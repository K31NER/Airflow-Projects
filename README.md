# ğŸŒ¦ï¸ ETL Weather Airflow

Este proyecto implementa un pipeline **ETL (Extract, Transform, Load)** automatizado y contenerizado utilizando **Apache Airflow** y **Docker**. Su objetivo principal es la ingesta continua de datos meteorolÃ³gicos desde la API de [Open-Meteo](https://open-meteo.com/), su procesamiento y posterior almacenamiento en una base de datos **PostgreSQL** (Neon DB).

## ğŸš€ CaracterÃ­sticas

*   **ExtracciÃ³n (Extract)**: Obtiene datos climÃ¡ticos en tiempo real (temperatura, humedad, precipitaciÃ³n, viento, etc.) para coordenadas geogrÃ¡ficas especÃ­ficas.
*   **TransformaciÃ³n (Transform)**: Limpia, estructura y valida los datos recibidos utilizando **Pydantic**, asegurando la calidad de la informaciÃ³n.
*   **Carga (Load)**: Inserta los datos procesados en una base de datos PostgreSQL utilizando **SQLModel** (ORM).
*   **OrquestaciÃ³n**: Utiliza **Apache Airflow** para programar y monitorear la ejecuciÃ³n diaria del flujo de trabajo.
*   **Infraestructura como CÃ³digo**: Todo el entorno se despliega fÃ¡cilmente mediante **Docker Compose**.

## ğŸ› ï¸ TecnologÃ­as

*   **Lenguaje**: Python 3.11+
*   **Orquestador**: Apache Airflow 2.9.0
*   **Contenedores**: Docker & Docker Compose
*   **Base de Datos**: PostgreSQL (Neon DB)
*   **LibrerÃ­as Clave**:
    *   `sqlmodel`: Para el modelado y gestiÃ³n de la base de datos.
    *   `pydantic`: Para la validaciÃ³n de esquemas de datos.
    *   `httpx`: Para realizar peticiones HTTP asÃ­ncronas a la API.

## ğŸ“‚ Estructura del Proyecto

```
AirFlow-docker/
â”œâ”€â”€ dags/                   # DAGs de Airflow (Punto de entrada)
â”‚   â””â”€â”€ dag_etl_weather.py
â”œâ”€â”€ etl_weather/            # MÃ³dulo principal del proyecto
â”‚   â”œâ”€â”€ db/                 # ConfiguraciÃ³n de conexiÃ³n a BD
â”‚   â”œâ”€â”€ ETL/                # LÃ³gica del proceso (Extract, Transform, Load)
â”‚   â”œâ”€â”€ model/              # Modelos de base de datos (Tablas)
â”‚   â”œâ”€â”€ schemas/            # Esquemas de datos (Pydantic)
â”‚   â””â”€â”€ main.py             # DefiniciÃ³n del DAG
â”œâ”€â”€ docker-compose.yaml     # ConfiguraciÃ³n de servicios Docker
â””â”€â”€ requirements.txt        # Dependencias del proyecto
```

## âš™ï¸ InstalaciÃ³n y Uso

### Prerrequisitos
*   Docker Desktop instalado y corriendo.
*   Una instancia de PostgreSQL (o una cuenta en Neon DB).

### Pasos para desplegar

1.  **Clonar el repositorio**:
    ```bash
    git clone <url-del-repositorio>
    cd AirFlow-docker
    ```

2.  **Configurar Variables de Entorno**:
    Crea un archivo `.env` en la raÃ­z del proyecto con la siguiente variable:
    ```env
    NEON_DB=postgresql://usuario:password@host:port/nombre_db
    AIRFLOW_UID=50000
    ```

3.  **Iniciar Airflow**:
    Ejecuta el siguiente comando para levantar los contenedores:
    ```bash
    docker-compose up -d
    ```

4.  **Acceder a la Interfaz**:
    *   Abre tu navegador en [http://localhost:8080](http://localhost:8080).
    *   **Usuario**: `airflow`
    *   **ContraseÃ±a**: `airflow`

5.  **Activar el DAG**:
    Busca el DAG llamado `dag_etl_weather` y actÃ­valo. Se ejecutarÃ¡ automÃ¡ticamente una vez al dÃ­a (`@daily`).

